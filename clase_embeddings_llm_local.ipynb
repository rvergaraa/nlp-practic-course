{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "d78b7276",
            "metadata": {},
            "source": [
                "# Clase Práctica: Embeddings, Búsqueda Semántica y LLMs Locales\n",
                "\n",
                "**Duración estimada:** 60-75 minutos\n",
                "\n",
                "En este notebook aprenderemos a:\n",
                "1.  Entender la diferencia fundamental entre vectorización clásica (Bag of Words) y Embeddings.\n",
                "2.  **Visualizar** embeddings en 2D para entender su agrupación semántica.\n",
                "3.  Implementar un sistema de búsqueda semántica simple.\n",
                "4.  Ejecutar un Gran Modelo de Lenguaje (LLM) localmente en Colab usando la GPU.\n",
                "5.  Construir un **RAG (Retrieval Augmented Generation)** simple combinando todo lo anterior."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "908a35e4",
            "metadata": {},
            "source": [
                "## 0. Configuración del Entorno (5 min)\n",
                "\n",
                "Asegúrate de estar ejecutando este notebook con un entorno de ejecución T4 GPU en Google Colab.\n",
                "`Entorno de ejecución > Cambiar tipo de entorno de ejecución > T4 GPU`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d58bb9f1",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Verificamos disponibilidad de GPU\n",
                "!nvidia-smi"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3f48500a",
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "# Instalación de librerías necesarias\n",
                "# matplotlib y scikit-learn son para visualización (PCA)\n",
                "!pip install -q sentence-transformers transformers torch accelerate bitsandbytes langchain pypdf matplotlib scikit-learn"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e85e117c",
            "metadata": {},
            "source": [
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "3ebcbbd9",
            "metadata": {},
            "source": [
                "## 1. Embeddings vs Vectorización Clásica (15 min)\n",
                "\n",
                "La vectorización clásica (como CountVectorizer o TF-IDF) se basa en la frecuencia de palabras exactas. Los Embeddings capturan el **significado semántico**."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "bb6f65a9",
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "from sklearn.feature_extraction.text import CountVectorizer\n",
                "from sentence_transformers import SentenceTransformer, util\n",
                "\n",
                "# Frases de ejemplo con significado similar pero palabras distintas\n",
                "frases = [\n",
                "    \"El perro corre en el jardín\",\n",
                "    \"El canino juega en el patio\",\n",
                "    \"The dog runs in the garden\",  # Inglés\n",
                "    \"Me gusta programar en Python\",\n",
                "    \"I love coding in Python\",\n",
                "    \"La astronomía estudia las estrellas\"\n",
                "]\n",
                "\n",
                "print(\"=== 1. Enfoque Clásico (Bag of Words) ===\")\n",
                "vectorizer = CountVectorizer()\n",
                "vectors_bow = vectorizer.fit_transform(frases)\n",
                "\n",
                "# Las columnas son palabras individuales. Notar la 'escasez' (muchos ceros).\n",
                "df_bow = pd.DataFrame(vectors_bow.toarray(), columns=vectorizer.get_feature_names_out())\n",
                "display(df_bow.head(3))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a50fbb94",
            "metadata": {},
            "source": [
                "### El Poder de los Embeddings Multilingües\n",
                "\n",
                "Ahora usaremos un modelo de Sentence Transformers entrenado para entender significado a través de múltiples idiomas."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c1dfbad0",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n=== 2. Enfoque Semántico (Embeddings) ===\")\n",
                "# Usamos un modelo multilingüe: paraphrase-multilingual-MiniLM-L12-v2\n",
                "model_multi = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
                "\n",
                "embeddings = model_multi.encode(frases)\n",
                "\n",
                "# Calculamos similitud entre 'El perro...' y 'The dog...'\n",
                "# Índices: 0 y 2\n",
                "similitud = util.cos_sim(embeddings[0], embeddings[2])\n",
                "print(f\"Similitud 'Perro' vs 'Dog': {similitud.item():.4f}\")\n",
                "\n",
                "similitud_random = util.cos_sim(embeddings[0], embeddings[5])\n",
                "print(f\"Similitud 'Perro' vs 'Astronomía': {similitud_random.item():.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f3a1c0b8",
            "metadata": {},
            "source": [
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "0bcd24cc",
            "metadata": {},
            "source": [
                "## 2. Visualización de Embeddings con PCA (10 min)\n",
                "\n",
                "Los embeddings suelen tener cientos de dimensiones (ej. 384 o 768). Los humanos solo vemos en 3D. Usaremos **PCA (Principal Component Analysis)** para reducir esas dimensiones a 2 y poder graficarlo."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0cf47c54",
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "from sklearn.decomposition import PCA\n",
                "\n",
                "# Generamos embeddings de una lista más variada\n",
                "lista_conceptos = [\n",
                "    \"perro\", \"gato\", \"caballo\", \"vaca\",   # Animales\n",
                "    \"dog\", \"cat\", \"horse\", \"cow\",         # Animales EN\n",
                "    \"coche\", \"moto\", \"camión\", \"avión\",   # Vehículos\n",
                "    \"car\", \"motorcycle\", \"truck\", \"plane\", # Vehículos EN\n",
                "    \"manzana\", \"pera\", \"plátano\", \"fruta\", # Comida\n",
                "    \"apple\", \"pear\", \"banana\", \"fruit\"     # Comida EN\n",
                "]\n",
                "\n",
                "emb_conceptos = model_multi.encode(lista_conceptos)\n",
                "\n",
                "# Reducir a 2 dimensiones\n",
                "pca = PCA(n_components=2)\n",
                "emb_2d = pca.fit_transform(emb_conceptos)\n",
                "\n",
                "# Graficar\n",
                "plt.figure(figsize=(10, 8))\n",
                "plt.scatter(emb_2d[:, 0], emb_2d[:, 1])\n",
                "\n",
                "for i, palabra in enumerate(lista_conceptos):\n",
                "    plt.annotate(palabra, xy=(emb_2d[i, 0], emb_2d[i, 1]))\n",
                "\n",
                "plt.title(\"Visualización de Embeddings con PCA\")\n",
                "plt.grid(True)\n",
                "plt.show()\n",
                "\n",
                "# NOTA: Observa cómo los conceptos similares (e idiomas) se agrupan juntos."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "54f591f2",
            "metadata": {},
            "source": [
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "6890e23c",
            "metadata": {},
            "source": [
                "## 3. Manejo de Documentos y Búsqueda Semántica (15 min)\n",
                "\n",
                "Chunking y recuperación de información."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c7504444",
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
                "\n",
                "texto_largo = \"\"\"\n",
                "La inteligencia artificial (IA) es un campo de la informática que busca crear sistemas capaces de imitar la inteligencia humana.\n",
                "El aprendizaje profundo (Deep Learning) utiliza redes neuronales artificiales.\n",
                "Los Grandes Modelos de Lenguaje (LLMs) como GPT-4, Claude o Llama 3 han revolucionado la interacción humano-máquina.\n",
                "Hugging Face es una comunidad y plataforma de ciencia de datos que proporciona herramientas para trabajar con ML.\n",
                "El procesamiento de lenguaje natural (NLP) permite a las máquinas entender texto y voz.\n",
                "RAG (Retrieval-Augmented Generation) mejora las respuestas de los LLMs inyectando datos externos.\n",
                "\"\"\"\n",
                "\n",
                "# Chunking\n",
                "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20)\n",
                "chunks = text_splitter.split_text(texto_largo)\n",
                "\n",
                "# Indexación (Crear embeddings solo una vez)\n",
                "corpus_embeddings = model_multi.encode(chunks)\n",
                "\n",
                "# Función de búsqueda\n",
                "def buscar_informacion(pregunta, top_k=2):\n",
                "    pregunta_emb = model_multi.encode(pregunta)\n",
                "    hits = util.semantic_search(pregunta_emb, corpus_embeddings, top_k=top_k)\n",
                "    \n",
                "    resultados = []\n",
                "    for hit in hits[0]:\n",
                "        id_doc = hit['corpus_id']\n",
                "        score = hit['score']\n",
                "        resultados.append((chunks[id_doc], score))\n",
                "    return resultados\n",
                "\n",
                "# Prueba\n",
                "print(buscar_informacion(\"¿Qué es RAG?\")[0])\n",
                "print(buscar_informacion(\"Plataforma de modelos de IA\")[0])"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f09a8d8d",
            "metadata": {},
            "source": [
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "2b8d0283",
            "metadata": {},
            "source": [
                "## 4. LLMs Locales con Hugging Face (15 min)\n",
                "\n",
                "Cargaremos un modelo optimizado (Quantized 4-bit) para generar texto."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8b1cb19e",
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
                "\n",
                "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
                "\n",
                "bnb_config = BitsAndBytesConfig(\n",
                "    load_in_4bit=True,\n",
                "    bnb_4bit_quant_type=\"nf4\",\n",
                "    bnb_4bit_compute_dtype=torch.float16\n",
                ")\n",
                "\n",
                "print(\"Cargando LLM...\")\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
                "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map=\"auto\")\n",
                "print(\"¡Modelo listo!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b2382b3c",
            "metadata": {},
            "source": [
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f09cab43",
            "metadata": {},
            "source": [
                "## 5. RAG: Retrieval Augmented Generation (10 min)\n",
                "\n",
                "Uniremos todo. Cuando el usuario hace una pregunta:\n",
                "1.  Buscamos los chunks más relevantes.\n",
                "2.  Construimos un prompt que incluya esa información.\n",
                "3.  Le pedimos al LLM que responda USANDO esa información."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "59491acf",
            "metadata": {},
            "outputs": [],
            "source": [
                "def sistema_rag(pregunta):\n",
                "    # 1. Recuperación (Retrieval)\n",
                "    contexto_relevante = buscar_informacion(pregunta, top_k=1)\n",
                "    texto_contexto = contexto_relevante[0][0] # Tomamos el texto del primer resultado\n",
                "    \n",
                "    # 2. Construcción del Prompt (Augmented)\n",
                "    # Instruimos al modelo para que use el contexto\n",
                "    prompt_final = f\"\"\"<|system|>\n",
                "Usa la siguiente información para responder a la pregunta del usuario. Si no sabes, di que no sabes.\n",
                "INFORMACIÓN: {texto_contexto}\n",
                "<|user|>\n",
                "PREGUNTA: {pregunta}\n",
                "<|assistant|>\n",
                "\"\"\"\n",
                "    \n",
                "    # 3. Generación (Generation)\n",
                "    inputs = tokenizer(prompt_final, return_tensors=\"pt\").to(\"cuda\")\n",
                "    outputs = model.generate(**inputs, max_new_tokens=150, temperature=0.7)\n",
                "    respuesta = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
                "    \n",
                "    # Limpieza básica para extraer solo la parte del asistente\n",
                "    if \"<|assistant|>\" in respuesta:\n",
                "        return respuesta.split(\"<|assistant|>\")[1].strip()\n",
                "    return respuesta\n",
                "\n",
                "# Probamos el sistema RAG\n",
                "print(\"Pregunta: ¿Para qué sirve RAG?\")\n",
                "print(\"Respuesta LLM:\", sistema_rag(\"¿Para qué sirve RAG?\"))\n",
                "\n",
                "print(\"\\nPregunta: ¿Qué es Hugging Face?\")\n",
                "print(\"Respuesta LLM:\", sistema_rag(\"¿Qué es Hugging Face?\"))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "03d33465",
            "metadata": {},
            "source": [
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "64da50e1",
            "metadata": {},
            "source": [
                "## 6. Ejercicios Prácticos (\"Your Turn\") (15 min)\n",
                "\n",
                "Ahora te toca a ti. Completa las siguientes celdas."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "cdc9e5a5",
            "metadata": {},
            "outputs": [],
            "source": [
                "# EJERCICIO 1: Nuevos Embeddings\n",
                "# Crea una lista de palabras que incluyan \"Tecnología\", \"Computadora\", \"Fútbol\", \"Deporte\".\n",
                "# Genera sus embeddings y calcula la similitud entre \"Computadora\" y \"Fútbol\".\n",
                "# ¿Qué valor esperas obtener? (Alto/Bajo)\n",
                "\n",
                "# Tu código aquí:\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "87e6d1c9",
            "metadata": {},
            "outputs": [],
            "source": [
                "# EJERCICIO 2: Prompting Creativo\n",
                "# Modifica la función 'sistema_rag' o usa el modelo directamente para que responda \n",
                "# como si fuera un Pirata del siglo XVII.\n",
                "\n",
                "prompt_pirata = \"<|system|> Eres un pirata rudo. <|user|> Explícame qué es la IA. <|assistant|>\"\n",
                "\n",
                "# Tu código aquí para generar:\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
