{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Clase Práctica: La Importancia de la Limpieza de Texto en NLP\n",
                "\n",
                "En este notebook aprenderemos **por qué** limpiar los datos es el paso más importante en cualquier proyecto de NLP. Veremos cómo el texto \"sucio\" infla nuestro vocabulario inútilmente y cómo técnicas simples pueden mejorar drásticamente la calidad de nuestros datos.\n",
                "\n",
                "## Objetivos\n",
                "1.  Identificar ruido en textos (HTML, emails, signos).\n",
                "2.  Normalizar texto (acentos, mayúsculas).\n",
                "3.  Comparar el tamaño del vocabulario antes y después de la limpieza.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup e Instalación\n",
                "Necesitaremos `spaCy` y su modelo en español."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install spacy sentence-transformers scikit-learn -q\n",
                "!python -m spacy download es_core_news_sm"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import spacy\n",
                "import re\n",
                "import unicodedata\n",
                "import pandas as pd\n",
                "from sklearn.feature_extraction.text import CountVectorizer\n",
                "\n",
                "# Cargar modelo de SpaCy en español\n",
                "nlp = spacy.load(\"es_core_news_sm\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. El Problema: Texto \"Sucio\" del Mundo Real"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "dirty_texts = [\n",
                "    \"¡HOLA! Estoy muy freliz con mi compra en www.tienda.com :)\",\n",
                "    \"Hola, estoy feliz??? con la compra... Contacto: soporte@email.com\",\n",
                "    \"Féliz de comprar aqui. #Recomendado 100%!!\",\n",
                "    \"No me GUSTA nada, quiero devolución\"\n",
                "]\n",
                "\n",
                "print(\"Ejemplos sucios:\", dirty_texts)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Análisis Inicial\n",
                "Si intentamos contar palabras ahora, veremos el caos."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "vectorizer = CountVectorizer()\n",
                "X = vectorizer.fit_transform(dirty_texts)\n",
                "\n",
                "print(f\"Tamaño del Vocabulario Sucio: {len(vectorizer.get_feature_names_out())}\")\n",
                "print(\"Algunas 'features':\", vectorizer.get_feature_names_out())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Observación:** Es probable que veas palabras separadas inútilmente o conectadas a signos, e incluso inconsistencias de mayúsculas."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. La Caja de Herramientas de Limpieza"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def clean_text(text):\n",
                "    # 1. Minúsculas\n",
                "    text = text.lower()\n",
                "    \n",
                "    # 2. Eliminar URLs y Emails con Regex\n",
                "    text = re.sub(r'http\\S+|www\\.\\S+', '', text)\n",
                "    text = re.sub(r'\\S+@\\S+', '', text)\n",
                "    \n",
                "    # 3. Eliminar caracteres especiales (manteniendo letras y espacios básicos)\n",
                "    # Forma simple: conservar solo alfanuméricos\n",
                "    # text = re.sub(r'[^a-zA-ZáéíóúüñÁÉÍÓÚÜÑ ]', '', text)\n",
                "    \n",
                "    # 4. Normalización de Unicode (Quitar tildes si se desea, opcional)\n",
                "    # text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8')\n",
                "    \n",
                "    # 5. Procesamiento con SpaCy (Lematización y Stopwords)\n",
                "    doc = nlp(text)\n",
                "    clean_tokens = []\n",
                "    for token in doc:\n",
                "        if not token.is_stop and not token.is_punct and not token.like_num:\n",
                "            # Usamos el lema (raíz) de la palabra\n",
                "            clean_tokens.append(token.lemma_)\n",
                "            \n",
                "    return \" \".join(clean_tokens)\n",
                "\n",
                "# Probemos con uno\n",
                "ejemplo = \"¡Los niños están corriendo rápido!\"\n",
                "print(f\"Original: {ejemplo}\")\n",
                "print(f\"Limpio:   {clean_text(ejemplo)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Midiendo el Impacto"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Aplicar limpieza a todo el corpus\n",
                "cleaned_texts = [clean_text(t) for t in dirty_texts]\n",
                "\n",
                "print(\"Textos Limpios:\")\n",
                "for t in cleaned_texts:\n",
                "    print(f\"- {t}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "vectorizer_clean = CountVectorizer()\n",
                "X_clean = vectorizer_clean.fit_transform(cleaned_texts)\n",
                "\n",
                "print(f\"Tamaño del Vocabulario Limpio: {len(vectorizer_clean.get_feature_names_out())}\")\n",
                "print(\"Features limpias:\", vectorizer_clean.get_feature_names_out())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Conclusión Visual\n",
                "Hemos reducido el ruido y unificado conceptos (ej. \"feliz\" y \"féliz\" o conjugaciones verbales si aplicamos lematización correctamente)."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}