# Gu√≠a Docente: Embeddings, B√∫squeda Sem√°ntica y LLMs Locales

**Archivo del Notebook:** `clase_embeddings_llm_local.ipynb`
**Duraci√≥n Estimada:** 60 - 75 minutos
**Requisitos T√©cnicos:** Google Colab con **T4 GPU** (Gratuito)

---

## üéØ Objetivos de Aprendizaje
Al finalizar la clase, los estudiantes ser√°n capaces de:
1.  **Diferenciar** conceptual y pr√°cticamente entre una vectorizaci√≥n cl√°sica (Bag of Words) y Embeddings sem√°nticos.
2.  **Visualizar** c√≥mo los modelos de lenguaje agrupan conceptos sem√°nticos similares en un espacio vectorial.
3.  **Implementar** un motor de b√∫squeda sem√°ntica simple utilizando `SentenceTransformers`.
4.  **Desplegar** un LLM local (TinyLlama) utilizando t√©cnicas de cuantizaci√≥n (4-bit) para optimizar recursos.
5.  **Integrar** b√∫squeda y generaci√≥n en un sistema RAG (Retrieval-Augmented Generation) b√°sico.

---

## ‚è±Ô∏è Cronograma de la Clase (Minuto a Minuto)

| Tiempo | Secci√≥n | Descripci√≥n y Puntos Clave |
| :--- | :--- | :--- |
| **00-05** | **0. Setup** | - Verificar que el entorno sea **T4 GPU** (`!nvidia-smi`).<br>- Explicar brevemente las librer√≠as: `transformers` (huggingface), `sentence-transformers` (embeddings), `bitsandbytes` (optimizaci√≥n). |
| **05-20** | **1. Embeddings vs BoW** | - **Teor√≠a**: BoW cuenta palabras, Embeddings capturan significado.<br>- **Demo**: Mostrar que `CountVectorizer` no ve relaci√≥n entre "Perro" y "Canino", pero los Embeddings s√≠.<br>- **Multiling√ºismo**: Destacar c√≥mo el modelo 'paraphrase-multilingual' conecta "Dog" con "Perro". |
| **20-30** | **2. Visualizaci√≥n (PCA)** | - Explicar que los embeddings tienen muchas dimensiones (ej. 384).<br>- Usar PCA para bajar a 2D.<br>- **Actividad visual**: Ver en el gr√°fico c√≥mo 'gato/perro' se separan de 'coche/moto'. |
| **30-45** | **3. B√∫squeda Sem√°ntica** | - Concepto de **Chunking**: Dividir textos largos para no saturar el modelo. <br>- Indexar un texto de juguete (sobre IA/HuggingFace).<br>- Hacer una *query* y mostrar el resultado m√°s relevante (`util.semantic_search`). |
| **45-60** | **4. LLMs Locales** | - Cargar `TinyLlama-1.1B`.<br>- **Concepto Clave**: Cuantizaci√≥n (4-bit). Explicar que esto permite correr modelos pesados en GPUs modestas.<br>- Generar un texto simple. |
| **60-70** | **5. RAG (Integraci√≥n)** | - Unir las piezas: Usar el chunk recuperado en la secci√≥n 3 como contexto para el prompt del LLM.<br>- Mostrar c√≥mo el LLM responde mejor cuando tiene "informaci√≥n fresca". |
| **70-75** | **6. Ejercicios ("Your Turn")** | - Dar tiempo a los alumnos para resolver los retos.<br>- **Reto 1**: Probar nuevas palabras para ver similitud.<br>- **Reto 2**: Cambiar la "sys prompt" del LLM (ej. modo Pirata). |

---

## üõ†Ô∏è Soluci√≥n de Problemas Comunes (Troubleshooting)

1.  **Error de Memoria (CUDA OOM)**:
    *   *Causa:* El estudiante ejecut√≥ celdas muy r√°pido o carg√≥ modelos grandes sin reiniciar.
    *   *Soluci√≥n:* `Entorno de ejecuci√≥n > Reiniciar sesi√≥n` y correr todo de nuevo asegurando que se usa la configuraci√≥n `bnb_config` (4-bit).

2.  **Lentitud en descargas**:
    *   La primera vez que se ejecuta `!pip install` y `model.encode`, descarga varios cientos de MB. Es normal que tome 2-3 minutos iniciales.

3.  **El gr√°fico PCA se ve amontonado**:
    *   Depende de la aleatoriedad de PCA y los ejemplos. Sugiere a los estudiantes agregar palabras muy distintas (ej. "Pizza", "Saturno") para ver mejor la separaci√≥n.

---

## üí° Sugerencias Pedag√≥gicas
*   En la secci√≥n de **Embeddings**, preg√∫ntales: *"¬øQu√© pasar√≠a si usamos una palabra que tiene dos significados, como 'Banco' (dinero vs asiento)?"*. (Respuesta: Los embeddings contextuales como BERT suelen manejarlo bien, pero modelos simples de oraciones a veces promedian el significado).
*   En la secci√≥n **RAG**, haz √©nfasis en que **esta es la base de aplicaciones modernas** como los chatbots de soporte t√©cnico o asistentes legales.
